{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Emoji Generator: Complete Pipeline\n",
        "## Training, Evaluation & Inference with Stable Diffusion\n",
        "\n",
        "This notebook provides a complete end-to-end pipeline for training, evaluating, and using a Stable Diffusion model for emoji generation. Designed to work in Google Colab and Kaggle environments.\n",
        "\n",
        "### Features:\n",
        "- Complete training pipeline without MLflow\n",
        "- Direct configuration (no YAML files)\n",
        "- Evaluation metrics and visualization\n",
        "- Inference and image generation\n",
        "- Google Colab & Kaggle compatible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Environment Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install transformers diffusers accelerate\n",
        "%pip install pillow matplotlib tqdm pandas numpy\n",
        "%pip install ipywidgets\n",
        "\n",
        "# For Colab: Mount Google Drive if needed\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Environment setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Configuration & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.amp import GradScaler, autocast\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "from typing import List, Optional, Tuple, Any, Dict\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Transformers and diffusers\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - Replace YAML config files\n",
        "# =============================================================================\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_CONFIG = {\n",
        "    \"stable_diffusion\": {\n",
        "        \"h_dim\": 384,\n",
        "        \"n_head\": 8,\n",
        "        \"time_dim\": 1280,\n",
        "        \"num_train_timesteps\": 1000,\n",
        "        \"beta_start\": 0.00085,\n",
        "        \"beta_end\": 0.012\n",
        "    },\n",
        "    \"clip\": {\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "        \"model_id\": \"openai/clip-vit-base-patch32\",\n",
        "        \"max_length\": 77\n",
        "    },\n",
        "    \"vae\": {\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "        \"model_id\": \"stabilityai/sd-vae-ft-mse\",\n",
        "        \"scaling_factor\": 0.18215\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training Configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"epochs\": 50,  # Reduced for notebook demo\n",
        "    \"batch_size\": 4,  # Reduced for memory constraints\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"eta_min\": 1e-5,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"mixed_precision\": True,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"save_every\": 10,\n",
        "    \"log_every\": 5\n",
        "}\n",
        "\n",
        "# Data Configuration\n",
        "DATA_CONFIG = {\n",
        "    \"data_dirs\": [\n",
        "        \"datasets/blobs_crawled_data\",\n",
        "        \"datasets/pepe_crawled_data\"\n",
        "    ],\n",
        "    \"image_size\": [32, 32],\n",
        "    \"latent_size\": [4, 4],\n",
        "    \"train_split\": 0.9,\n",
        "    \"val_split\": 0.1,\n",
        "    \"num_workers\": 2,\n",
        "    \"pin_memory\": True,\n",
        "    \"persistent_workers\": False  # Set to False for notebooks\n",
        "}\n",
        "\n",
        "# Inference Configuration\n",
        "INFERENCE_CONFIG = {\n",
        "    \"num_inference_steps\": 25,  # Reduced for faster inference\n",
        "    \"guidance_scale\": 7.5,\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "# Device setup\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "set_seed(42)\n",
        "print(\"Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Architecture Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SCHEDULER COMPONENT\n",
        "# =============================================================================\n",
        "\n",
        "def embed_timesteps(timesteps: torch.Tensor, embedding_dim: int = 320) -> torch.Tensor:\n",
        "    \"\"\"Embed timesteps for diffusion process\"\"\"\n",
        "    half_dim = embedding_dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(10000) * torch.arange(half_dim, dtype=torch.float32) / half_dim\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None, :]\n",
        "    return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "\n",
        "\n",
        "class DDPMScheduler:\n",
        "    \"\"\"DDPM Scheduler for diffusion process\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        random_generator: torch.Generator,\n",
        "        train_timesteps: int = 1000,\n",
        "        beta_start: float = 0.00085,\n",
        "        beta_end: float = 0.012,\n",
        "    ):\n",
        "        self.betas = (\n",
        "            torch.linspace(\n",
        "                beta_start**0.5, beta_end**0.5, train_timesteps, dtype=torch.float32\n",
        "            ) ** 2\n",
        "        )\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.one_val = torch.tensor(1.0)\n",
        "        self.generator = random_generator\n",
        "        self.total_train_timesteps = train_timesteps\n",
        "        self.timesteps = torch.from_numpy(np.arange(0, train_timesteps)[::-1].copy())\n",
        "\n",
        "    def set_steps(self, num_inference_steps: int = 50):\n",
        "        self.num_inference_steps = num_inference_steps\n",
        "        step_ratio = self.total_train_timesteps // num_inference_steps\n",
        "        timesteps = (\n",
        "            (np.arange(0, num_inference_steps) * step_ratio)\n",
        "            .round()[::-1]\n",
        "            .copy()\n",
        "            .astype(np.int64)\n",
        "        )\n",
        "        self.timesteps = torch.from_numpy(timesteps)\n",
        "\n",
        "    def _get_prev_timestep(self, timestep: int) -> int:\n",
        "        return timestep - self.total_train_timesteps // self.num_inference_steps\n",
        "\n",
        "    def _get_variance(self, timestep: int) -> torch.Tensor:\n",
        "        prev_t = self._get_prev_timestep(timestep)\n",
        "        alpha_cumprod_t = self.alphas_cumprod[timestep]\n",
        "        alpha_cumprod_t_prev = (\n",
        "            self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one_val\n",
        "        )\n",
        "        beta_t = 1 - alpha_cumprod_t / alpha_cumprod_t_prev\n",
        "        variance = (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * beta_t\n",
        "        return torch.clamp(variance, min=1e-20)\n",
        "\n",
        "    def step(\n",
        "        self, timestep: int, latents: torch.Tensor, model_output: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        t = timestep\n",
        "        prev_t = self._get_prev_timestep(t)\n",
        "\n",
        "        alpha_cumprod_t = self.alphas_cumprod[t]\n",
        "        alpha_cumprod_t_prev = (\n",
        "            self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one_val\n",
        "        )\n",
        "        beta_cumprod_t = 1 - alpha_cumprod_t\n",
        "        beta_cumprod_t_prev = 1 - alpha_cumprod_t_prev\n",
        "        alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev\n",
        "        beta_t = 1 - alpha_t\n",
        "\n",
        "        # Predict original sample\n",
        "        pred_original_sample = (\n",
        "            latents - beta_cumprod_t**0.5 * model_output\n",
        "        ) / alpha_cumprod_t**0.5\n",
        "\n",
        "        # Compute coefficients\n",
        "        pred_original_sample_coeff = (\n",
        "            alpha_cumprod_t_prev**0.5 * beta_t\n",
        "        ) / beta_cumprod_t\n",
        "        current_sample_coeff = alpha_t**0.5 * beta_cumprod_t_prev / beta_cumprod_t\n",
        "\n",
        "        # Compute predicted previous sample\n",
        "        pred_prev_sample = (\n",
        "            pred_original_sample_coeff * pred_original_sample\n",
        "            + current_sample_coeff * latents\n",
        "        )\n",
        "\n",
        "        # Add noise\n",
        "        variance = 0\n",
        "        if t > 0:\n",
        "            device = model_output.device\n",
        "            noise = torch.randn(\n",
        "                model_output.shape,\n",
        "                generator=self.generator,\n",
        "                device=device,\n",
        "                dtype=model_output.dtype,\n",
        "            )\n",
        "            variance = (self._get_variance(t) ** 0.5) * noise\n",
        "\n",
        "        return pred_prev_sample + variance\n",
        "\n",
        "    def add_noise(\n",
        "        self, original_samples: torch.Tensor, timesteps: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        alphas_cumprod = self.alphas_cumprod.to(\n",
        "            device=original_samples.device, dtype=original_samples.dtype\n",
        "        )\n",
        "        timesteps = timesteps.to(original_samples.device)\n",
        "\n",
        "        sqrt_alpha_cumprod = alphas_cumprod[timesteps] ** 0.5\n",
        "        sqrt_alpha_cumprod = sqrt_alpha_cumprod.view(\n",
        "            sqrt_alpha_cumprod.shape[0], *([1] * (original_samples.ndim - 1))\n",
        "        )\n",
        "\n",
        "        sqrt_one_minus_alpha_cumprod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.view(\n",
        "            sqrt_one_minus_alpha_cumprod.shape[0], *([1] * (original_samples.ndim - 1))\n",
        "        )\n",
        "\n",
        "        noise = torch.randn(\n",
        "            original_samples.shape,\n",
        "            generator=self.generator,\n",
        "            device=original_samples.device,\n",
        "            dtype=original_samples.dtype,\n",
        "        )\n",
        "\n",
        "        noisy_samples = (\n",
        "            sqrt_alpha_cumprod * original_samples + sqrt_one_minus_alpha_cumprod * noise\n",
        "        )\n",
        "        return noisy_samples, noise\n",
        "\n",
        "print(\"Scheduler components defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ATTENTION COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention mechanism\"\"\"\n",
        "    def __init__(self, n_heads: int, d_embed: int, in_proj_bias: bool = True):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x: torch.Tensor, causal_mask: bool = False) -> torch.Tensor:\n",
        "        input_shape = x.shape\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "        if causal_mask:\n",
        "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "        output = weight @ v\n",
        "        output = output.transpose(1, 2).reshape(input_shape)\n",
        "        return self.out_proj(output)\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Cross-attention for text conditioning\"\"\"\n",
        "    def __init__(self, n_heads: int, d_embed: int, d_cross: int, in_proj_bias: bool = True):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
        "        self.k_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.v_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "        input_shape = x.shape\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(y)\n",
        "        v = self.v_proj(y)\n",
        "\n",
        "        q = q.view(batch_size, sequence_length, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "        output = weight @ v\n",
        "        output = output.transpose(1, 2).reshape(input_shape)\n",
        "        return self.out_proj(output)\n",
        "\n",
        "print(\"Attention components defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# UNET COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"Time embedding for diffusion timesteps\"\"\"\n",
        "    def __init__(self, n_embd: int):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.proj2 = nn.Linear(4 * n_embd, 4 * n_embd)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.proj1(x)\n",
        "        x = F.silu(x)\n",
        "        x = self.proj2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNETResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for UNET\"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_dim: int = 1280):\n",
        "        super().__init__()\n",
        "        self.gn_feature = nn.GroupNorm(32, in_channels)\n",
        "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.time_embedding_proj = nn.Linear(time_dim, out_channels)\n",
        "        \n",
        "        self.gn_merged = nn.GroupNorm(32, out_channels)\n",
        "        self.conv_merged = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        \n",
        "        if in_channels == out_channels:\n",
        "            self.residual_connection = nn.Identity()\n",
        "        else:\n",
        "            self.residual_connection = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, input_feature: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:\n",
        "        residual = input_feature\n",
        "        \n",
        "        h = self.gn_feature(input_feature)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv_feature(h)\n",
        "        \n",
        "        time_emb_processed = F.silu(time_emb)\n",
        "        time_emb_projected = self.time_embedding_proj(time_emb_processed)\n",
        "        time_emb_projected = time_emb_projected.unsqueeze(-1).unsqueeze(-1)\n",
        "        \n",
        "        merged_feature = h + time_emb_projected\n",
        "        merged_feature = self.gn_merged(merged_feature)\n",
        "        merged_feature = F.silu(merged_feature)\n",
        "        merged_feature = self.conv_merged(merged_feature)\n",
        "        \n",
        "        return merged_feature + self.residual_connection(residual)\n",
        "\n",
        "\n",
        "class UNETAttentionBlock(nn.Module):\n",
        "    \"\"\"Attention block for UNET\"\"\"\n",
        "    def __init__(self, num_heads: int, head_dim: int, context_dim: int = 512):\n",
        "        super().__init__()\n",
        "        embed_dim = num_heads * head_dim\n",
        "        \n",
        "        self.gn_in = nn.GroupNorm(32, embed_dim, eps=1e-6)\n",
        "        self.proj_in = nn.Conv2d(embed_dim, embed_dim, kernel_size=1, padding=0)\n",
        "        \n",
        "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn_1 = SelfAttention(num_heads, embed_dim, in_proj_bias=False)\n",
        "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
        "        self.attn_2 = CrossAttention(num_heads, embed_dim, context_dim, in_proj_bias=False)\n",
        "        self.ln_3 = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        self.ffn_geglu = nn.Linear(embed_dim, 4 * embed_dim * 2)\n",
        "        self.ffn_out = nn.Linear(4 * embed_dim, embed_dim)\n",
        "        self.proj_out = nn.Conv2d(embed_dim, embed_dim, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, context_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        skip_connection = input_tensor\n",
        "        \n",
        "        B, C, H, W = input_tensor.shape\n",
        "        HW = H * W\n",
        "        \n",
        "        h = self.gn_in(input_tensor)\n",
        "        h = self.proj_in(h)\n",
        "        h = h.view(B, C, HW).transpose(-1, -2)\n",
        "        \n",
        "        # Self-attention\n",
        "        attn1_skip = h\n",
        "        h = self.ln_1(h)\n",
        "        h = self.attn_1(h)\n",
        "        h = h + attn1_skip\n",
        "        \n",
        "        # Cross-attention\n",
        "        attn2_skip = h\n",
        "        h = self.ln_2(h)\n",
        "        h = self.attn_2(h, context_tensor)\n",
        "        h = h + attn2_skip\n",
        "        \n",
        "        # Feed forward with GEGLU\n",
        "        ffn_skip = h\n",
        "        h = self.ln_3(h)\n",
        "        h, gate = self.ffn_geglu(h).chunk(2, dim=-1)\n",
        "        h = h * F.gelu(gate)\n",
        "        h = self.ffn_out(h)\n",
        "        h = h + ffn_skip\n",
        "        \n",
        "        h = h.transpose(-1, -2).view(B, C, H, W)\n",
        "        return self.proj_out(h) + skip_connection\n",
        "\n",
        "print(\"UNET components defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE MODELS\n",
        "# =============================================================================\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    \"\"\"Simplified UNET for emoji generation\"\"\"\n",
        "    def __init__(self, h_dim: int = 384, n_head: int = 8):\n",
        "        super().__init__()\n",
        "        head_dim = h_dim // n_head\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_conv = nn.Conv2d(4, h_dim, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Encoder layers\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            UNETResidualBlock(h_dim, h_dim),\n",
        "            UNETAttentionBlock(n_head, head_dim),\n",
        "            UNETResidualBlock(h_dim, h_dim),\n",
        "        ])\n",
        "        \n",
        "        # Middle block\n",
        "        self.middle_block = UNETAttentionBlock(n_head, head_dim)\n",
        "        \n",
        "        # Decoder layers  \n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            UNETResidualBlock(h_dim, h_dim),\n",
        "            UNETAttentionBlock(n_head, head_dim),\n",
        "            UNETResidualBlock(h_dim, h_dim),\n",
        "        ])\n",
        "\n",
        "    def forward(self, latent: torch.Tensor, context: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.input_conv(latent)\n",
        "        \n",
        "        # Encoder\n",
        "        skip_connections = []\n",
        "        for i, block in enumerate(self.encoder_blocks):\n",
        "            if isinstance(block, UNETResidualBlock):\n",
        "                h = block(h, time_emb)\n",
        "            else:  # UNETAttentionBlock\n",
        "                h = block(h, context)\n",
        "            skip_connections.append(h)\n",
        "        \n",
        "        # Middle\n",
        "        h = self.middle_block(h, context)\n",
        "        \n",
        "        # Decoder\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            h = h + skip_connections.pop()\n",
        "            if isinstance(block, UNETResidualBlock):\n",
        "                h = block(h, time_emb)\n",
        "            else:  # UNETAttentionBlock\n",
        "                h = block(h, context)\n",
        "        \n",
        "        return h\n",
        "\n",
        "\n",
        "class UNETOutputLayer(nn.Module):\n",
        "    \"\"\"Output layer for UNET\"\"\"\n",
        "    def __init__(self, h_dim: int, output_channels: int):\n",
        "        super().__init__()\n",
        "        self.gn = nn.GroupNorm(32, h_dim)\n",
        "        self.conv = nn.Conv2d(h_dim, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.gn(x)\n",
        "        x = F.silu(x)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class StableDiffusion(nn.Module):\n",
        "    \"\"\"Complete Stable Diffusion model\"\"\"\n",
        "    def __init__(self, h_dim: int = 384, n_head: int = 8):\n",
        "        super().__init__()\n",
        "        self.time_embedding = TimeEmbedding(320)\n",
        "        self.unet = UNET(h_dim, n_head)\n",
        "        self.unet_output = UNETOutputLayer(h_dim, 4)\n",
        "\n",
        "    def forward(self, latent: torch.Tensor, context: torch.Tensor, timestep: torch.Tensor) -> torch.Tensor:\n",
        "        time_emb = embed_timesteps(timestep).to(latent.device)\n",
        "        time_emb = self.time_embedding(time_emb)\n",
        "        output = self.unet(latent, context, time_emb)\n",
        "        return self.unet_output(output)\n",
        "\n",
        "\n",
        "class CLIPTextEncoder(nn.Module):\n",
        "    \"\"\"CLIP text encoder for conditioning\"\"\"\n",
        "    def __init__(self, model_id: str = \"openai/clip-vit-base-patch32\", device: str = \"cuda\"):\n",
        "        super().__init__()\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
        "        self.device = device\n",
        "        \n",
        "        # Freeze parameters\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.text_encoder.eval()\n",
        "        self.text_encoder.to(device)\n",
        "\n",
        "    def forward(self, prompts: List[str]) -> torch.Tensor:\n",
        "        inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        \n",
        "        input_ids = inputs.input_ids.to(self.device)\n",
        "        attention_mask = inputs.attention_mask.to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            text_encoder_output = self.text_encoder(\n",
        "                input_ids=input_ids, attention_mask=attention_mask\n",
        "            )\n",
        "        \n",
        "        return text_encoder_output.last_hidden_state\n",
        "\n",
        "\n",
        "class VAEEncoder(nn.Module):\n",
        "    \"\"\"VAE encoder for latent space conversion\"\"\"\n",
        "    def __init__(self, model_id: str = \"stabilityai/sd-vae-ft-mse\", scaling_factor: float = 0.18215):\n",
        "        super().__init__()\n",
        "        self.vae = AutoencoderKL.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
        "        self.scaling_factor = scaling_factor\n",
        "        \n",
        "        # Freeze VAE parameters\n",
        "        self.vae.requires_grad_(False)\n",
        "        self.vae.eval()\n",
        "\n",
        "    def encode(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            latents = self.vae.encode(images).latent_dist.sample()\n",
        "            return latents * self.scaling_factor\n",
        "\n",
        "    def decode(self, latents: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            latents = latents / self.scaling_factor\n",
        "            return self.vae.decode(latents).sample\n",
        "\n",
        "print(\"All model components defined!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Data Loading & Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA LOADING\n",
        "# =============================================================================\n",
        "\n",
        "class EmojiDataset(Dataset):\n",
        "    \"\"\"Dataset for emoji images and text prompts\"\"\"\n",
        "    def __init__(self, data_dirs: List[str], transform: Optional[transforms.Compose] = None):\n",
        "        dataframes = []\n",
        "        for data_dir in data_dirs:\n",
        "            csv_file = os.path.join(data_dir, \"metadata.csv\")\n",
        "            image_folder = os.path.join(data_dir, \"images\")\n",
        "            \n",
        "            if os.path.exists(csv_file):\n",
        "                df = pd.read_csv(csv_file)\n",
        "                df[\"image_path\"] = df[\"file_name\"].astype(str).str.replace(\"\\\\\", \"/\")\n",
        "                df[\"full_image_path\"] = df[\"image_path\"].apply(\n",
        "                    lambda x: os.path.join(image_folder, x)\n",
        "                )\n",
        "                dataframes.append(df)\n",
        "        \n",
        "        if dataframes:\n",
        "            self.dataframe = pd.concat(dataframes, ignore_index=True)\n",
        "        else:\n",
        "            # Create dummy data for demo purposes\n",
        "            print(\"Warning: No data found, creating dummy dataset\")\n",
        "            self.dataframe = pd.DataFrame({\n",
        "                'prompt': ['happy emoji', 'sad emoji', 'angry emoji', 'surprised emoji'] * 25,\n",
        "                'full_image_path': ['dummy_path'] * 100\n",
        "            })\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.prompts = self.dataframe[\"prompt\"].tolist()\n",
        "        self.image_paths = self.dataframe[\"full_image_path\"].tolist()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
        "        image_path = self.image_paths[idx]\n",
        "        prompt = self.prompts[idx].replace('\"', \"\").replace(\"'\", \"\")\n",
        "        \n",
        "        # Handle dummy data or missing images\n",
        "        if image_path == 'dummy_path' or not os.path.exists(image_path):\n",
        "            # Create a random RGB image for demo\n",
        "            image = Image.fromarray(np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8))\n",
        "        else:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, prompt\n",
        "\n",
        "\n",
        "def get_transforms(image_size: Tuple[int, int] = (32, 32)) -> transforms.Compose:\n",
        "    \"\"\"Get image transforms\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "\n",
        "def create_data_loaders(data_config: Dict[str, Any], training_config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Create train and validation data loaders\"\"\"\n",
        "    transform = get_transforms(tuple(data_config[\"image_size\"]))\n",
        "    \n",
        "    dataset = EmojiDataset(\n",
        "        data_dirs=data_config[\"data_dirs\"],\n",
        "        transform=transform,\n",
        "    )\n",
        "    \n",
        "    train_size = int(data_config[\"train_split\"] * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42),\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=training_config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=data_config[\"num_workers\"],\n",
        "        pin_memory=data_config[\"pin_memory\"],\n",
        "        persistent_workers=data_config[\"persistent_workers\"],\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=training_config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=data_config[\"num_workers\"],\n",
        "        pin_memory=data_config[\"pin_memory\"],\n",
        "        persistent_workers=data_config[\"persistent_workers\"],\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "print(\"Creating data loaders...\")\n",
        "train_loader, val_loader = create_data_loaders(DATA_CONFIG, TRAINING_CONFIG)\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Test data loading\n",
        "try:\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    images, prompts = sample_batch\n",
        "    print(f\"Sample batch shape: {images.shape}\")\n",
        "    print(f\"Sample prompts: {prompts[:2]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Data loading test failed: {e}\")\n",
        "\n",
        "print(\"Data loading setup complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Training Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "class StableDiffusionTrainer:\n",
        "    \"\"\"Trainer class for Stable Diffusion model\"\"\"\n",
        "    def __init__(self, model_config: Dict[str, Any], training_config: Dict[str, Any], device: str = \"cuda\"):\n",
        "        self.model_config = model_config\n",
        "        self.training_config = training_config\n",
        "        self.device = device\n",
        "        \n",
        "        # Initialize models\n",
        "        self.diffusion_model = StableDiffusion(\n",
        "            h_dim=model_config[\"stable_diffusion\"][\"h_dim\"],\n",
        "            n_head=model_config[\"stable_diffusion\"][\"n_head\"],\n",
        "        ).to(device)\n",
        "        \n",
        "        self.text_encoder = CLIPTextEncoder(\n",
        "            model_id=model_config[\"clip\"][\"model_id\"], \n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        self.vae_encoder = VAEEncoder(\n",
        "            model_id=model_config[\"vae\"][\"model_id\"],\n",
        "            scaling_factor=model_config[\"vae\"][\"scaling_factor\"]\n",
        "        ).to(device)\n",
        "        \n",
        "        # Initialize scheduler\n",
        "        self.generator = torch.Generator(device=device)\n",
        "        self.scheduler = DDPMScheduler(\n",
        "            random_generator=self.generator,\n",
        "            train_timesteps=model_config[\"stable_diffusion\"][\"num_train_timesteps\"],\n",
        "            beta_start=model_config[\"stable_diffusion\"][\"beta_start\"],\n",
        "            beta_end=model_config[\"stable_diffusion\"][\"beta_end\"],\n",
        "        )\n",
        "        \n",
        "        # Initialize optimizer and scheduler\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.diffusion_model.parameters(),\n",
        "            lr=training_config[\"learning_rate\"],\n",
        "            weight_decay=training_config[\"weight_decay\"],\n",
        "        )\n",
        "        \n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            self.optimizer,\n",
        "            T_max=training_config[\"epochs\"],\n",
        "            eta_min=training_config[\"eta_min\"],\n",
        "        )\n",
        "        \n",
        "        # Mixed precision training\n",
        "        self.scaler = GradScaler() if training_config[\"mixed_precision\"] else None\n",
        "        \n",
        "        # Loss function\n",
        "        self.criterion = nn.MSELoss()\n",
        "        \n",
        "        # Training state\n",
        "        self.current_epoch = 0\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.diffusion_model.train()\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = len(train_loader)\n",
        "        \n",
        "        progress_bar = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Epoch {self.current_epoch + 1}/{self.training_config['epochs']}\",\n",
        "        )\n",
        "        \n",
        "        for batch_idx, (images, prompts) in enumerate(progress_bar):\n",
        "            images = images.to(self.device)\n",
        "            \n",
        "            # Encode images to latent space\n",
        "            with torch.no_grad():\n",
        "                latents = self.vae_encoder.encode(images)\n",
        "            \n",
        "            # Sample random timesteps\n",
        "            timesteps = torch.randint(\n",
        "                0,\n",
        "                self.scheduler.total_train_timesteps,\n",
        "                (latents.shape[0],),\n",
        "                device=self.device,\n",
        "            )\n",
        "            \n",
        "            # Add noise to latents\n",
        "            noisy_latents, noise = self.scheduler.add_noise(latents, timesteps)\n",
        "            \n",
        "            # Encode text\n",
        "            with torch.no_grad():\n",
        "                text_embeddings = self.text_encoder(prompts)\n",
        "            \n",
        "            # Forward pass with mixed precision\n",
        "            if self.scaler:\n",
        "                with autocast(\"cuda\", dtype=torch.float16):\n",
        "                    noise_pred = self.diffusion_model(\n",
        "                        noisy_latents, text_embeddings, timesteps\n",
        "                    )\n",
        "                    loss = self.criterion(noise_pred, noise)\n",
        "            else:\n",
        "                noise_pred = self.diffusion_model(\n",
        "                    noisy_latents, text_embeddings, timesteps\n",
        "                )\n",
        "                loss = self.criterion(noise_pred, noise)\n",
        "            \n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            if self.scaler:\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            \n",
        "            # Update progress\n",
        "            batch_loss = loss.item()\n",
        "            epoch_loss += batch_loss\n",
        "            self.global_step += 1\n",
        "            \n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{batch_loss:.5f}\",\n",
        "                \"lr\": f\"{self.optimizer.param_groups[0]['lr']:.6f}\",\n",
        "            })\n",
        "        \n",
        "        return epoch_loss / num_batches\n",
        "\n",
        "    def validate(self, val_loader: DataLoader) -> float:\n",
        "        \"\"\"Validate the model\"\"\"\n",
        "        self.diffusion_model.eval()\n",
        "        val_loss = 0.0\n",
        "        num_batches = len(val_loader)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, prompts in tqdm(val_loader, desc=\"Validation\"):\n",
        "                images = images.to(self.device)\n",
        "                \n",
        "                # Encode images to latent space\n",
        "                latents = self.vae_encoder.encode(images)\n",
        "                \n",
        "                # Sample random timesteps\n",
        "                timesteps = torch.randint(\n",
        "                    0,\n",
        "                    self.scheduler.total_train_timesteps,\n",
        "                    (latents.shape[0],),\n",
        "                    device=self.device,\n",
        "                )\n",
        "                \n",
        "                # Add noise to latents\n",
        "                noisy_latents, noise = self.scheduler.add_noise(latents, timesteps)\n",
        "                \n",
        "                # Encode text\n",
        "                text_embeddings = self.text_encoder(prompts)\n",
        "                \n",
        "                # Forward pass\n",
        "                noise_pred = self.diffusion_model(\n",
        "                    noisy_latents, text_embeddings, timesteps\n",
        "                )\n",
        "                loss = self.criterion(noise_pred, noise)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        return val_loss / num_batches\n",
        "\n",
        "    def save_checkpoint(self, filepath: str, is_best: bool = False):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            \"epoch\": self.current_epoch,\n",
        "            \"model_state_dict\": self.diffusion_model.state_dict(),\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"lr_scheduler_state_dict\": self.lr_scheduler.state_dict(),\n",
        "            \"best_loss\": self.best_loss,\n",
        "            \"train_losses\": self.train_losses,\n",
        "            \"val_losses\": self.val_losses,\n",
        "            \"model_config\": self.model_config,\n",
        "            \"training_config\": self.training_config,\n",
        "        }\n",
        "        \n",
        "        if self.scaler:\n",
        "            checkpoint[\"scaler_state_dict\"] = self.scaler.state_dict()\n",
        "        \n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        torch.save(checkpoint, filepath)\n",
        "        \n",
        "        if is_best:\n",
        "            best_path = filepath.replace('.pt', '_best.pt')\n",
        "            torch.save(checkpoint, best_path)\n",
        "\n",
        "    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None):\n",
        "        \"\"\"Complete training loop\"\"\"\n",
        "        print(f\"Starting training for {self.training_config['epochs']} epochs...\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.diffusion_model.parameters() if p.requires_grad)}\")\n",
        "        \n",
        "        for epoch in range(self.current_epoch, self.training_config[\"epochs\"]):\n",
        "            self.current_epoch = epoch\n",
        "            \n",
        "            # Train epoch\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            \n",
        "            # Validation\n",
        "            if val_loader and epoch % 5 == 0:  # Validate every 5 epochs\n",
        "                val_loss = self.validate(val_loader)\n",
        "                self.val_losses.append(val_loss)\n",
        "                \n",
        "                # Save best model\n",
        "                if val_loss < self.best_loss:\n",
        "                    self.best_loss = val_loss\n",
        "                    self.save_checkpoint(f\"checkpoints/emoji_sd_epoch_{epoch}_best.pt\", is_best=True)\n",
        "                \n",
        "                print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.5f}\")\n",
        "            \n",
        "            # Save checkpoint\n",
        "            if epoch % self.training_config[\"save_every\"] == 0:\n",
        "                self.save_checkpoint(f\"checkpoints/emoji_sd_epoch_{epoch}.pt\")\n",
        "            \n",
        "            # Update learning rate\n",
        "            self.lr_scheduler.step()\n",
        "        \n",
        "        # Save final model\n",
        "        self.save_checkpoint(f\"checkpoints/emoji_sd_final.pt\")\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "print(\"Training pipeline defined!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Inference Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INFERENCE PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "def rescale_tensor(tensor: torch.Tensor, from_range: Tuple[float, float], to_range: Tuple[float, float], clamp: bool = False) -> torch.Tensor:\n",
        "    \"\"\"Rescale tensor from one range to another\"\"\"\n",
        "    from_min, from_max = from_range\n",
        "    to_min, to_max = to_range\n",
        "    \n",
        "    # Rescale\n",
        "    tensor = (tensor - from_min) / (from_max - from_min)\n",
        "    tensor = tensor * (to_max - to_min) + to_min\n",
        "    \n",
        "    if clamp:\n",
        "        tensor = torch.clamp(tensor, min=to_min, max=to_max)\n",
        "    \n",
        "    return tensor\n",
        "\n",
        "\n",
        "class EmojiGenerator:\n",
        "    \"\"\"Emoji generator using trained Stable Diffusion model\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model: StableDiffusion,\n",
        "        text_encoder: CLIPTextEncoder,\n",
        "        vae_encoder: VAEEncoder,\n",
        "        scheduler: DDPMScheduler,\n",
        "        device: str = \"cuda\",\n",
        "    ):\n",
        "        self.diffusion_model = diffusion_model.eval()\n",
        "        self.text_encoder = text_encoder\n",
        "        self.vae_encoder = vae_encoder\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        num_inference_steps: int = 25,\n",
        "        guidance_scale: float = 7.5,\n",
        "        height: int = 32,\n",
        "        width: int = 32,\n",
        "        seed: Optional[int] = None,\n",
        "        batch_size: int = 1,\n",
        "    ) -> List[Image.Image]:\n",
        "        \"\"\"Generate emoji images from text prompt\"\"\"\n",
        "        \n",
        "        # Set random seed\n",
        "        generator = torch.Generator(device=self.device)\n",
        "        if seed is not None:\n",
        "            generator.manual_seed(seed)\n",
        "\n",
        "        # Encode text\n",
        "        prompts = [prompt] * batch_size\n",
        "        text_embeddings = self.text_encoder(prompts)\n",
        "\n",
        "        # Classifier-free guidance: create unconditional embeddings\n",
        "        unconditional_prompts = [\"\"] * batch_size\n",
        "        uncond_embeddings = self.text_encoder(unconditional_prompts)\n",
        "\n",
        "        # Concatenate for classifier-free guidance\n",
        "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # Set scheduler timesteps\n",
        "        self.scheduler.set_steps(num_inference_steps)\n",
        "\n",
        "        # Create random latents\n",
        "        latents_shape = (batch_size, 4, height // 8, width // 8)\n",
        "        latents = torch.randn(latents_shape, generator=generator, device=self.device)\n",
        "\n",
        "        # Denoising loop\n",
        "        for t in tqdm(self.scheduler.timesteps, desc=\"Generating\"):\n",
        "            # Expand latents for classifier-free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "            # Predict noise\n",
        "            noise_pred = self.diffusion_model(latent_model_input, text_embeddings, t)\n",
        "\n",
        "            # Perform classifier-free guidance\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (\n",
        "                noise_pred_text - noise_pred_uncond\n",
        "            )\n",
        "\n",
        "            # Compute previous noisy sample\n",
        "            latents = self.scheduler.step(t, latents, noise_pred)\n",
        "\n",
        "        # Decode latents to images\n",
        "        images = self.vae_encoder.decode(latents)\n",
        "\n",
        "        # Convert to PIL Images\n",
        "        images = rescale_tensor(images, (-1, 1), (0, 255), clamp=True)\n",
        "        images = images.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        pil_images = [Image.fromarray(img) for img in images]\n",
        "        return pil_images\n",
        "\n",
        "    @classmethod\n",
        "    def from_checkpoint(cls, checkpoint_path: str, device: str = \"cuda\") -> \"EmojiGenerator\":\n",
        "        \"\"\"Load generator from checkpoint\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        \n",
        "        model_config = checkpoint[\"model_config\"]\n",
        "        \n",
        "        # Load models\n",
        "        diffusion_model = StableDiffusion(\n",
        "            h_dim=model_config[\"stable_diffusion\"][\"h_dim\"],\n",
        "            n_head=model_config[\"stable_diffusion\"][\"n_head\"],\n",
        "        ).to(device)\n",
        "        \n",
        "        diffusion_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        \n",
        "        text_encoder = CLIPTextEncoder(\n",
        "            model_id=model_config[\"clip\"][\"model_id\"], \n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        vae_encoder = VAEEncoder(\n",
        "            model_id=model_config[\"vae\"][\"model_id\"],\n",
        "            scaling_factor=model_config[\"vae\"][\"scaling_factor\"]\n",
        "        ).to(device)\n",
        "        \n",
        "        # Create scheduler\n",
        "        generator = torch.Generator(device=device)\n",
        "        scheduler = DDPMScheduler(\n",
        "            random_generator=generator,\n",
        "            train_timesteps=model_config[\"stable_diffusion\"][\"num_train_timesteps\"],\n",
        "            beta_start=model_config[\"stable_diffusion\"][\"beta_start\"],\n",
        "            beta_end=model_config[\"stable_diffusion\"][\"beta_end\"],\n",
        "        )\n",
        "        \n",
        "        return cls(diffusion_model, text_encoder, vae_encoder, scheduler, device)\n",
        "\n",
        "\n",
        "def plot_images(images: List[Image.Image], prompts: List[str] = None, cols: int = 4):\n",
        "    \"\"\"Plot generated images\"\"\"\n",
        "    rows = (len(images) + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
        "    \n",
        "    if rows == 1:\n",
        "        axes = [axes] if cols == 1 else axes\n",
        "    elif cols == 1:\n",
        "        axes = [[ax] for ax in axes]\n",
        "    \n",
        "    for i, img in enumerate(images):\n",
        "        row, col = i // cols, i % cols\n",
        "        ax = axes[row][col] if rows > 1 else axes[col]\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        if prompts and i < len(prompts):\n",
        "            ax.set_title(prompts[i][:30], fontsize=8)\n",
        "    \n",
        "    # Hide empty subplots\n",
        "    for i in range(len(images), rows * cols):\n",
        "        row, col = i // cols, i % cols\n",
        "        ax = axes[row][col] if rows > 1 else axes[col]\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Inference pipeline defined!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Execute Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = StableDiffusionTrainer(MODEL_CONFIG, TRAINING_CONFIG, DEVICE)\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "print(\"Note: This is a demonstration with reduced epochs and batch size.\")\n",
        "print(\"For full training, increase epochs to 300+ and batch_size based on your GPU memory.\")\n",
        "\n",
        "# Uncomment the next line to start training\n",
        "# trainer.train(train_loader, val_loader)\n",
        "\n",
        "print(\"Training setup complete! Uncomment the trainer.train() line to start training.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Training Visualization & Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING MONITORING\n",
        "# =============================================================================\n",
        "\n",
        "def plot_training_curves(trainer: StableDiffusionTrainer):\n",
        "    \"\"\"Plot training and validation loss curves\"\"\"\n",
        "    if not trainer.train_losses:\n",
        "        print(\"No training data to plot yet.\")\n",
        "        return\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Training loss\n",
        "    ax1.plot(trainer.train_losses, label='Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Validation loss (if available)\n",
        "    if trainer.val_losses:\n",
        "        ax2.plot(range(0, len(trainer.train_losses), 5), trainer.val_losses, label='Validation Loss', color='orange')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_title('Validation Loss')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'No validation data', transform=ax2.transAxes, ha='center', va='center')\n",
        "        ax2.set_title('Validation Loss (Not Available)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def monitor_training_progress(trainer: StableDiffusionTrainer):\n",
        "    \"\"\"Monitor current training progress\"\"\"\n",
        "    print(f\"Current Epoch: {trainer.current_epoch}\")\n",
        "    print(f\"Global Step: {trainer.global_step}\")\n",
        "    print(f\"Best Loss: {trainer.best_loss:.6f}\")\n",
        "    \n",
        "    if trainer.train_losses:\n",
        "        print(f\"Latest Training Loss: {trainer.train_losses[-1]:.6f}\")\n",
        "    \n",
        "    if trainer.val_losses:\n",
        "        print(f\"Latest Validation Loss: {trainer.val_losses[-1]:.6f}\")\n",
        "\n",
        "\n",
        "# Example usage (after training has started)\n",
        "print(\"Training monitoring functions defined.\")\n",
        "print(\"Use plot_training_curves(trainer) and monitor_training_progress(trainer) to track progress.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_model(trainer: StableDiffusionTrainer, val_loader: DataLoader) -> Dict[str, float]:\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    print(\"Evaluating model...\")\n",
        "    \n",
        "    # Basic validation loss\n",
        "    val_loss = trainer.validate(val_loader)\n",
        "    \n",
        "    metrics = {\n",
        "        \"validation_loss\": val_loss,\n",
        "        \"best_loss\": trainer.best_loss,\n",
        "        \"epochs_trained\": trainer.current_epoch,\n",
        "    }\n",
        "    \n",
        "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
        "    print(f\"Best Loss: {trainer.best_loss:.6f}\")\n",
        "    print(f\"Epochs Trained: {trainer.current_epoch}\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def generate_evaluation_samples(checkpoint_path: str = None, trainer: StableDiffusionTrainer = None):\n",
        "    \"\"\"Generate sample images for qualitative evaluation\"\"\"\n",
        "    print(\"Generating evaluation samples...\")\n",
        "    \n",
        "    # Create generator\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        generator = EmojiGenerator.from_checkpoint(checkpoint_path, DEVICE)\n",
        "    elif trainer:\n",
        "        # Create generator from current trainer\n",
        "        generator = EmojiGenerator(\n",
        "            trainer.diffusion_model,\n",
        "            trainer.text_encoder, \n",
        "            trainer.vae_encoder,\n",
        "            trainer.scheduler,\n",
        "            DEVICE\n",
        "        )\n",
        "    else:\n",
        "        print(\"No trained model available for evaluation.\")\n",
        "        return\n",
        "    \n",
        "    # Sample prompts for evaluation\n",
        "    evaluation_prompts = [\n",
        "        \"happy smiling emoji\",\n",
        "        \"sad crying emoji\", \n",
        "        \"angry red face emoji\",\n",
        "        \"surprised emoji with wide eyes\",\n",
        "        \"laughing emoji with tears\",\n",
        "        \"winking emoji\",\n",
        "        \"heart eyes emoji\",\n",
        "        \"thinking emoji\"\n",
        "    ]\n",
        "    \n",
        "    generated_images = []\n",
        "    \n",
        "    for prompt in tqdm(evaluation_prompts, desc=\"Generating samples\"):\n",
        "        try:\n",
        "            images = generator.generate(\n",
        "                prompt=prompt,\n",
        "                num_inference_steps=INFERENCE_CONFIG[\"num_inference_steps\"],\n",
        "                guidance_scale=INFERENCE_CONFIG[\"guidance_scale\"],\n",
        "                seed=42  # Fixed seed for reproducible evaluation\n",
        "            )\n",
        "            generated_images.extend(images)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating for prompt '{prompt}': {e}\")\n",
        "            # Create a placeholder image\n",
        "            placeholder = Image.new('RGB', (32, 32), color='gray')\n",
        "            generated_images.append(placeholder)\n",
        "    \n",
        "    # Plot generated samples\n",
        "    if generated_images:\n",
        "        print(\"Generated evaluation samples:\")\n",
        "        plot_images(generated_images, evaluation_prompts, cols=4)\n",
        "        \n",
        "        # Save samples\n",
        "        os.makedirs(\"evaluation_samples\", exist_ok=True)\n",
        "        for i, (img, prompt) in enumerate(zip(generated_images, evaluation_prompts)):\n",
        "            img.save(f\"evaluation_samples/sample_{i:02d}_{prompt.replace(' ', '_')}.png\")\n",
        "        \n",
        "        print(\"Evaluation samples saved to 'evaluation_samples/' directory.\")\n",
        "    else:\n",
        "        print(\"No images were generated successfully.\")\n",
        "\n",
        "\n",
        "# Evaluation functions are ready\n",
        "print(\"Evaluation functions defined.\")\n",
        "print(\"Use evaluate_model(trainer, val_loader) for quantitative evaluation.\")\n",
        "print(\"Use generate_evaluation_samples(trainer=trainer) for qualitative evaluation.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Interactive Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INTERACTIVE INFERENCE\n",
        "# =============================================================================\n",
        "\n",
        "def interactive_generation(checkpoint_path: str = None, trainer: StableDiffusionTrainer = None):\n",
        "    \"\"\"Interactive emoji generation function\"\"\"\n",
        "    \n",
        "    # Create generator\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading model from checkpoint: {checkpoint_path}\")\n",
        "        generator = EmojiGenerator.from_checkpoint(checkpoint_path, DEVICE)\n",
        "    elif trainer:\n",
        "        print(\"Using current trainer model...\")\n",
        "        generator = EmojiGenerator(\n",
        "            trainer.diffusion_model,\n",
        "            trainer.text_encoder,\n",
        "            trainer.vae_encoder, \n",
        "            trainer.scheduler,\n",
        "            DEVICE\n",
        "        )\n",
        "    else:\n",
        "        print(\"No trained model available. Please train a model first or provide a checkpoint path.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Interactive Emoji Generator Ready!\")\n",
        "    print(\"Enter text prompts to generate emojis. Type 'quit' to exit.\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\nEnter prompt: \").strip()\n",
        "            \n",
        "            if prompt.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if not prompt:\n",
        "                print(\"Please enter a valid prompt.\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"Generating emoji for: '{prompt}'...\")\n",
        "            \n",
        "            # Generate images\n",
        "            images = generator.generate(\n",
        "                prompt=prompt,\n",
        "                num_inference_steps=INFERENCE_CONFIG[\"num_inference_steps\"],\n",
        "                guidance_scale=INFERENCE_CONFIG[\"guidance_scale\"],\n",
        "                seed=None,  # Random seed for variety\n",
        "                batch_size=4  # Generate 4 variations\n",
        "            )\n",
        "            \n",
        "            # Display results\n",
        "            print(\"Generated emojis:\")\n",
        "            plot_images(images, [prompt] * len(images), cols=2)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGeneration interrupted.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "\n",
        "\n",
        "def batch_generation(prompts: List[str], checkpoint_path: str = None, trainer: StableDiffusionTrainer = None):\n",
        "    \"\"\"Generate emojis for a batch of prompts\"\"\"\n",
        "    \n",
        "    # Create generator\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        generator = EmojiGenerator.from_checkpoint(checkpoint_path, DEVICE)\n",
        "    elif trainer:\n",
        "        generator = EmojiGenerator(\n",
        "            trainer.diffusion_model,\n",
        "            trainer.text_encoder,\n",
        "            trainer.vae_encoder,\n",
        "            trainer.scheduler,\n",
        "            DEVICE\n",
        "        )\n",
        "    else:\n",
        "        print(\"No trained model available.\")\n",
        "        return []\n",
        "    \n",
        "    all_images = []\n",
        "    all_prompts = []\n",
        "    \n",
        "    print(f\"Generating emojis for {len(prompts)} prompts...\")\n",
        "    \n",
        "    for prompt in tqdm(prompts, desc=\"Batch generation\"):\n",
        "        try:\n",
        "            images = generator.generate(\n",
        "                prompt=prompt,\n",
        "                num_inference_steps=INFERENCE_CONFIG[\"num_inference_steps\"],\n",
        "                guidance_scale=INFERENCE_CONFIG[\"guidance_scale\"],\n",
        "                seed=42  # Fixed seed for consistency\n",
        "            )\n",
        "            all_images.extend(images)\n",
        "            all_prompts.extend([prompt] * len(images))\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating for '{prompt}': {e}\")\n",
        "    \n",
        "    if all_images:\n",
        "        print(\"Batch generation complete!\")\n",
        "        plot_images(all_images, all_prompts, cols=4)\n",
        "        \n",
        "        # Save batch results\n",
        "        os.makedirs(\"batch_generation\", exist_ok=True)\n",
        "        for i, (img, prompt) in enumerate(zip(all_images, all_prompts)):\n",
        "            safe_prompt = prompt.replace(' ', '_').replace('/', '_')\n",
        "            img.save(f\"batch_generation/batch_{i:03d}_{safe_prompt}.png\")\n",
        "        \n",
        "        print(\"Batch results saved to 'batch_generation/' directory.\")\n",
        "    \n",
        "    return all_images\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sample_prompts = [\n",
        "    \"happy emoji\",\n",
        "    \"sad emoji\", \n",
        "    \"angry emoji\",\n",
        "    \"laughing emoji\",\n",
        "    \"heart eyes emoji\"\n",
        "]\n",
        "\n",
        "print(\"Inference functions ready!\")\n",
        "print(\"Use interactive_generation() for interactive mode.\")\n",
        "print(\"Use batch_generation(prompts) for batch processing.\")\n",
        "print(f\"Example: batch_generation({sample_prompts})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL STATUS CHECK\n",
        "# =============================================================================\n",
        "\n",
        "print(\" Emoji Generator Pipeline Complete! \")\n",
        "print(\"=\" * 50)\n",
        "print(\" All components loaded successfully\")\n",
        "print(\" Data loaders created\")\n",
        "print(\" Model architecture defined\")\n",
        "print(\" Training pipeline ready\")\n",
        "print(\" Evaluation functions ready\") \n",
        "print(\" Inference pipeline ready\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n Quick Action Items:\")\n",
        "print(\"1. Uncomment 'trainer.train(train_loader, val_loader)' to start training\")\n",
        "print(\"2. Use 'plot_training_curves(trainer)' to monitor progress\")\n",
        "print(\"3. Run 'generate_evaluation_samples(trainer=trainer)' for qualitative evaluation\")\n",
        "print(\"4. Use 'interactive_generation(trainer=trainer)' for interactive inference\")\n",
        "\n",
        "print(f\"\\n Current Configuration:\")\n",
        "print(f\"   - Device: {DEVICE}\")\n",
        "print(f\"   - Model dimension: {MODEL_CONFIG['stable_diffusion']['h_dim']}\")\n",
        "print(f\"   - Training epochs: {TRAINING_CONFIG['epochs']}\")\n",
        "print(f\"   - Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"   - Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
        "\n",
        "print(\"\\n Ready for Google Colab and Kaggle!\")\n",
        "print(\" Tip: Increase epochs and batch_size for production training\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "emo-generator-ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
